{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4acd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import IPython\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pypandoc\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import typing\n",
    "import us\n",
    "import zipfile\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data_pipeline.utils import remove_all_from_dir, get_excel_column_name\n",
    "from data_pipeline.etl.base import ExtractTransformLoad\n",
    "from data_pipeline.etl.sources.census.etl_utils import get_state_information\n",
    "from data_pipeline.etl.sources.ejscreen_areas_of_concern.etl import (\n",
    "    EJSCREENAreasOfConcernETL,\n",
    ")\n",
    "\n",
    "\n",
    "from data_pipeline.score import field_names\n",
    "\n",
    "# Turn on TQDM for pandas so that we can have progress bars when running `apply`.\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3170c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress scientific notation in pandas (this shows up for census tract IDs)\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# Set some global parameters\n",
    "DATA_DIR = pathlib.Path.cwd().parent / \"data\"\n",
    "TEMP_DATA_DIR = DATA_DIR / \"tmp\"\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "COMPARISON_OUTPUTS_DIR = DATA_DIR / \"comparison_outputs\" / time_str\n",
    "\n",
    "# Make the dirs if they don't exist\n",
    "TEMP_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "COMPARISON_OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CEJST_PRIORITY_COMMUNITY_THRESHOLD = 0.75\n",
    "\n",
    "# Name fields using variables. (This makes it easy to reference the same fields frequently without using strings\n",
    "# and introducing the risk of misspelling the field name.)\n",
    "GEOID_STATE_FIELD_NAME = \"GEOID10_STATE\"\n",
    "COUNTRY_FIELD_NAME = \"Country\"\n",
    "\n",
    "# Define some suffixes\n",
    "POPULATION_SUFFIX = \" (priority population)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd39090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CEJST score data\n",
    "cejst_data_path = DATA_DIR / \"score\" / \"csv\" / \"full\" / \"usa.csv\"\n",
    "cejst_df = pd.read_csv(\n",
    "    cejst_data_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "# Create the state ID by taking the first two digits of the FIPS CODE of the tract.\n",
    "# For more information, see https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html.\n",
    "cejst_df.loc[:, GEOID_STATE_FIELD_NAME] = (\n",
    "    cejst_df.loc[:, ExtractTransformLoad.GEOID_TRACT_FIELD_NAME]\n",
    "    .astype(str)\n",
    "    .str[0:2]\n",
    ")\n",
    "\n",
    "cejst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251a0fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load EJSCREEN Areas of Concern data.\n",
    "\n",
    "# Load EJ Screen Areas of Concern\n",
    "# Before attempting, check whether or not the EJSCREEN AoC data is available locally.\n",
    "# Note: this data is provided privately and is not currently publicly available.\n",
    "# To enable the ETL code for EJSCREEN AoCs to run appropriately whether or not the person\n",
    "# running it has access to that data, `ejscreen_areas_of_concern_data_exists` checks whether the source file exists.\n",
    "# If it does exist, code can and should include this data. If it does not exist, code should\n",
    "# not reference this data.\n",
    "ejscreen_areas_of_concern_df: pd.DataFrame = None\n",
    "\n",
    "if EJSCREENAreasOfConcernETL.ejscreen_areas_of_concern_data_exists():\n",
    "    print(\"Loading EJSCREEN Areas of Concern data for score pipeline.\")\n",
    "    ejscreen_areas_of_concern_csv = (\n",
    "        DATA_DIR / \"dataset\" / \"ejscreen_areas_of_concern\" / \"usa.csv\"\n",
    "    )\n",
    "    ejscreen_areas_of_concern_df = pd.read_csv(\n",
    "        ejscreen_areas_of_concern_csv,\n",
    "        dtype={ExtractTransformLoad.GEOID_FIELD_NAME: \"string\"},\n",
    "        low_memory=False,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"EJSCREEN areas of concern data does not exist locally. Not attempting to load data into comparison tool.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a9e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge EJSCREEN AoCs into CEJST data.\n",
    "# Before attempting, check whether or not the EJSCREEN AoC data is available locally.\n",
    "if EJSCREENAreasOfConcernETL.ejscreen_areas_of_concern_data_exists():\n",
    "    # If available, merge EJSCREEN AoC data into CBG dfs.\n",
    "    # TODO: When we get AoC data at the tract level, fix this.\n",
    "    # Right now commenting this out to avoid merging CBG-level areas of concern on a tract-level CEJST definition.\n",
    "    # cejst_df = cejst_df.merge(\n",
    "    #     ejscreen_areas_of_concern_df, on=GEOID_FIELD_NAME, how=\"outer\"\n",
    "    # )\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "\n",
    "cejst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0dc2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyze one field at a time (useful for setting thresholds)\n",
    "\n",
    "quantile = 0.95\n",
    "\n",
    "for field in [\n",
    "    field_names.COLLEGE_ATTENDANCE_FIELD,\n",
    "    field_names.HIGH_SCHOOL_ED_FIELD,\n",
    "    field_names.MEDIAN_INCOME_AS_PERCENT_OF_AMI_FIELD,\n",
    "    field_names.POVERTY_LESS_THAN_200_FPL_FIELD,\n",
    "]:\n",
    "    print(f\"\\n~~~~Analysis for field `{field}`~~~~\")\n",
    "    print(cejst_df[field].describe())\n",
    "    print(\n",
    "        f\"\\nThere are {cejst_df[field].isnull().sum() * 100 / len(cejst_df):.2f}% of values missing.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nQuantile at {quantile} is {np.nanquantile(a=cejst_df[field], q=quantile)}\"\n",
    "    )\n",
    "    print(cejst_df[field].hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e462c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CalEnviroScreen 4.0\n",
    "CALENVIROSCREEN_SCORE_FIELD = \"calenviroscreen_score\"\n",
    "CALENVIROSCREEN_PERCENTILE_FIELD = \"calenviroscreen_percentile\"\n",
    "CALENVIROSCREEN_PRIORITY_COMMUNITY_FIELD = \"calenviroscreen_priority_community\"\n",
    "\n",
    "calenviroscreen_data_path = (\n",
    "    DATA_DIR / \"dataset\" / \"calenviroscreen4\" / \"data06.csv\"\n",
    ")\n",
    "calenviroscreen_df = pd.read_csv(\n",
    "    calenviroscreen_data_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "# Convert priority community field to a bool.\n",
    "calenviroscreen_df[\n",
    "    CALENVIROSCREEN_PRIORITY_COMMUNITY_FIELD\n",
    "] = calenviroscreen_df[CALENVIROSCREEN_PRIORITY_COMMUNITY_FIELD].astype(bool)\n",
    "\n",
    "calenviroscreen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec43dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load persistent poverty data\n",
    "persistent_poverty_path = (\n",
    "    DATA_DIR / \"dataset\" / \"persistent_poverty\" / \"usa.csv\"\n",
    ")\n",
    "persistent_poverty_df = pd.read_csv(\n",
    "    persistent_poverty_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "# Since \"Persistent Poverty Census Tract\" is labeled in both the score file (at the CBG level) and this tract file,\n",
    "# rename this field so it's easy to access the tract-level scores directly.\n",
    "\n",
    "PERSISTENT_POVERTY_TRACT_LEVEL_FIELD = \"Persistent Poverty, Tract Level\"\n",
    "PERSISTENT_POVERTY_CBG_LEVEL_FIELD = \"Persistent Poverty Census Tract\"\n",
    "\n",
    "persistent_poverty_df.rename(\n",
    "    columns={\n",
    "        PERSISTENT_POVERTY_CBG_LEVEL_FIELD: PERSISTENT_POVERTY_TRACT_LEVEL_FIELD\n",
    "    },\n",
    "    inplace=True,\n",
    "    errors=\"raise\",\n",
    ")\n",
    "\n",
    "persistent_poverty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81826d29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load mapping inequality data\n",
    "HOLC_FACTORS = [\n",
    "    field_names.HOLC_GRADE_D_TRACT_20_PERCENT_FIELD,\n",
    "    field_names.HOLC_GRADE_D_TRACT_50_PERCENT_FIELD,\n",
    "    field_names.HOLC_GRADE_D_TRACT_75_PERCENT_FIELD,\n",
    "]\n",
    "mapping_inequality_path = (\n",
    "    DATA_DIR / \"dataset\" / \"mapping_inequality\" / \"usa.csv\"\n",
    ")\n",
    "mapping_inequality_df = pd.read_csv(\n",
    "    mapping_inequality_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "mapping_inequality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_svi_index_path = DATA_DIR / \"dataset\" / \"cdc_svi_index\" / \"usa.csv\"\n",
    "cdc_svi_index_df = pd.read_csv(\n",
    "    cdc_svi_index_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "cdc_svi_index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c290efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Maryland EJScreen\n",
    "maryland_ejscreen_data_path = (\n",
    "    DATA_DIR / \"dataset\" / \"maryland_ejscreen\" / \"maryland.csv\"\n",
    ")\n",
    "maryland_ejscreen_df = pd.read_csv(\n",
    "    maryland_ejscreen_data_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "maryland_ejscreen_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605af1ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load alternative energy-related definition\n",
    "energy_definition_alternative_draft_path = (\n",
    "    DATA_DIR / \"dataset\" / \"energy_definition_alternative_draft\" / \"usa.csv\"\n",
    ")\n",
    "energy_definition_alternative_draft_df = pd.read_csv(\n",
    "    energy_definition_alternative_draft_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "energy_definition_alternative_draft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a2939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Michigan EJSCREEN\n",
    "michigan_ejscreen_data_path = (\n",
    "    DATA_DIR / \"dataset\" / \"michigan_ejscreen\" / \"michigan_ejscreen.csv\"\n",
    ")\n",
    "michigan_ejscreen_df = pd.read_csv(\n",
    "    michigan_ejscreen_data_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "michigan_ejscreen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39342aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EPA RSEI EJSCREEN\n",
    "epa_rsei_data_path = DATA_DIR / \"dataset\" / \"epa_rsei\" / \"usa.csv\"\n",
    "epa_rsei_df = pd.read_csv(\n",
    "    epa_rsei_data_path,\n",
    "    dtype={ExtractTransformLoad.GEOID_TRACT_FIELD_NAME: \"string\"},\n",
    ")\n",
    "\n",
    "epa_rsei_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65659c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join all dataframes that use tracts\n",
    "census_tract_dfs = [\n",
    "    cejst_df,\n",
    "    calenviroscreen_df,\n",
    "    persistent_poverty_df,\n",
    "    mapping_inequality_df,\n",
    "    epa_rsei_df,\n",
    "    maryland_ejscreen_df,\n",
    "    energy_definition_alternative_draft_df,\n",
    "    michigan_ejscreen_df,\n",
    "    cdc_svi_index_df,\n",
    "]\n",
    "\n",
    "merged_df = functools.reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left=left,\n",
    "        right=right,\n",
    "        on=ExtractTransformLoad.GEOID_TRACT_FIELD_NAME,\n",
    "        how=\"outer\",\n",
    "    ),\n",
    "    census_tract_dfs,\n",
    ")\n",
    "\n",
    "tract_values = (\n",
    "    merged_df[ExtractTransformLoad.GEOID_TRACT_FIELD_NAME].str.len().unique()\n",
    ")\n",
    "if any(tract_values != [11]):\n",
    "    print(tract_values)\n",
    "    raise ValueError(\"Some of the census tract data has the wrong length.\")\n",
    "\n",
    "if len(merged_df) > ExtractTransformLoad.EXPECTED_MAX_CENSUS_TRACTS:\n",
    "    raise ValueError(f\"Too many rows in the join: {len(merged_df)}.\")\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de78f71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Special handling for HOLC.\n",
    "# Fill in the null HOLC values as `False`. Otherwise the comparison tool will not run comparisons in states\n",
    "# without HOLC scores, and for HOLC, we'd like to see it across the whole US.\n",
    "for holc_factor in HOLC_FACTORS:\n",
    "    merged_df[holc_factor] = merged_df[holc_factor].fillna(False)\n",
    "\n",
    "merged_df[HOLC_FACTORS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c0f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a namedtuple for indices.\n",
    "Index = collections.namedtuple(\n",
    "    typename=\"Index\",\n",
    "    field_names=[\n",
    "        \"method_name\",\n",
    "        \"priority_communities_field\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define the indices used for CEJST scoring (`census_block_group_indices`) as well as comparison\n",
    "# (`census_tract_indices`).\n",
    "definition_l_factors = [\n",
    "    field_names.L_CLIMATE,\n",
    "    field_names.L_ENERGY,\n",
    "    field_names.L_TRANSPORTATION,\n",
    "    field_names.L_HOUSING,\n",
    "    field_names.L_POLLUTION,\n",
    "    field_names.L_WATER,\n",
    "    field_names.L_HEALTH,\n",
    "    field_names.L_WORKFORCE,\n",
    "    # Also include a combined factor for all the non-workforce elements.\n",
    "    field_names.L_NON_WORKFORCE,\n",
    "]\n",
    "\n",
    "definition_m_factors = [\n",
    "    field_names.M_CLIMATE,\n",
    "    field_names.M_ENERGY,\n",
    "    field_names.M_TRANSPORTATION,\n",
    "    field_names.M_HOUSING,\n",
    "    field_names.M_POLLUTION,\n",
    "    field_names.M_WATER,\n",
    "    field_names.M_HEALTH,\n",
    "    field_names.M_WORKFORCE,\n",
    "    # Also include a combined factor for all the non-workforce elements.\n",
    "    field_names.M_NON_WORKFORCE,\n",
    "]\n",
    "\n",
    "census_tract_indices = (\n",
    "    [\n",
    "        Index(\n",
    "            method_name=\"Definition M\",\n",
    "            priority_communities_field=field_names.SCORE_M_COMMUNITIES,\n",
    "        ),\n",
    "    ]\n",
    "    + [\n",
    "        Index(\n",
    "            method_name=\"Definition L\",\n",
    "            priority_communities_field=field_names.SCORE_L_COMMUNITIES,\n",
    "        ),\n",
    "    ]\n",
    "    # Insert indices for each of the factors from Definition M.\n",
    "    # Note: since these involve no renaming, we write them using list comprehension.\n",
    "    + [\n",
    "        Index(\n",
    "            method_name=factor,\n",
    "            priority_communities_field=factor,\n",
    "        )\n",
    "        for factor in definition_m_factors\n",
    "    ]\n",
    "    # Insert indices for each of the factors from Definition L.\n",
    "    # Note: since these involve no renaming, we write them using list comprehension.\n",
    "    + [\n",
    "        Index(\n",
    "            method_name=factor,\n",
    "            priority_communities_field=factor,\n",
    "        )\n",
    "        for factor in definition_l_factors\n",
    "    ]\n",
    "    + [\n",
    "        Index(\n",
    "            # Note: we're renaming Score G as NMTC Modified for clarity, since that's what Score G is under the hood.\n",
    "            method_name=\"NMTC Modified\",\n",
    "            priority_communities_field=field_names.SCORE_G_COMMUNITIES,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"NMTC\",\n",
    "            priority_communities_field=\"NMTC (communities)\",\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Score C\",\n",
    "            priority_communities_field=\"Score C (top 25th percentile)\",\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Score D (30th percentile)\",\n",
    "            priority_communities_field=\"Score D (top 30th percentile)\",\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Score D (25th percentile)\",\n",
    "            priority_communities_field=\"Score D (top 25th percentile)\",\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Score F\",\n",
    "            priority_communities_field=field_names.SCORE_F_COMMUNITIES,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"CalEnviroScreen 4.0\",\n",
    "            priority_communities_field=\"calenviroscreen_priority_community\",\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"EPA RSEI Aggregate Microdata\",\n",
    "            priority_communities_field=field_names.EPA_RSEI_SCORE_THRESHOLD_FIELD,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Persistent Poverty\",\n",
    "            priority_communities_field=PERSISTENT_POVERTY_TRACT_LEVEL_FIELD,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Maryland EJSCREEN\",\n",
    "            priority_communities_field=field_names.MARYLAND_EJSCREEN_BURDENED_THRESHOLD_FIELD,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=field_names.ENERGY_RELATED_COMMUNITIES_DEFINITION_ALTERNATIVE,\n",
    "            priority_communities_field=field_names.ENERGY_RELATED_COMMUNITIES_DEFINITION_ALTERNATIVE,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"CDC SVI Index\",\n",
    "            priority_communities_field=field_names.CDC_SVI_INDEX_THEMES_PRIORITY_COMMUNITY,\n",
    "        ),\n",
    "        Index(\n",
    "            method_name=\"Michigan EJSCREEN\",\n",
    "            priority_communities_field=field_names.MICHIGAN_EJSCREEN_PRIORITY_COMMUNITY_FIELD,\n",
    "        ),\n",
    "    ]\n",
    "    # Insert indices for each of the HOLC factors.\n",
    "    # Note: since these involve no renaming, we write them using list comprehension.\n",
    "    + [\n",
    "        Index(\n",
    "            method_name=factor,\n",
    "            priority_communities_field=factor,\n",
    "        )\n",
    "        for factor in HOLC_FACTORS\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ejscreen_areas_of_concern_census_block_group_indices = [\n",
    "    Index(\n",
    "        method_name=\"EJSCREEN Areas of Concern, National, 80th percentile\",\n",
    "        priority_communities_field=field_names.EJSCREEN_AREAS_OF_CONCERN_NATIONAL_80TH_PERCENTILE_COMMUNITIES_FIELD,\n",
    "    ),\n",
    "    Index(\n",
    "        method_name=\"EJSCREEN Areas of Concern, National, 90th percentile\",\n",
    "        priority_communities_field=field_names.EJSCREEN_AREAS_OF_CONCERN_NATIONAL_90TH_PERCENTILE_COMMUNITIES_FIELD,\n",
    "    ),\n",
    "    Index(\n",
    "        method_name=\"EJSCREEN Areas of Concern, National, 95th percentile\",\n",
    "        priority_communities_field=field_names.EJSCREEN_AREAS_OF_CONCERN_NATIONAL_95TH_PERCENTILE_COMMUNITIES_FIELD,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Before including EJSCREEN AoC indicators are included, check whether or not the EJSCREEN AoC data is available locally.\n",
    "if EJSCREENAreasOfConcernETL.ejscreen_areas_of_concern_data_exists():\n",
    "    # Add EJSCREEN AoCs to all of the CBG indices.\n",
    "    # TODO: When we get AoC data at the tract level, fix this.\n",
    "    # Right now commenting this out to avoid merging CBG-level areas of concern on a tract-level CEJST definition.\n",
    "    # census_block_group_indices.extend(\n",
    "    #     ejscreen_areas_of_concern_census_block_group_indices\n",
    "    # )\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# These fields will be used for statistical comparisons.\n",
    "comparison_fields = [\n",
    "    field_names.POVERTY_LESS_THAN_100_FPL_FIELD,\n",
    "    field_names.POVERTY_LESS_THAN_200_FPL_FIELD,\n",
    "    field_names.COLLEGE_ATTENDANCE_FIELD,\n",
    "    field_names.MEDIAN_INCOME_AS_PERCENT_OF_AMI_FIELD,\n",
    "    field_names.LINGUISTIC_ISO_FIELD,\n",
    "    field_names.UNEMPLOYMENT_FIELD,\n",
    "    field_names.HIGH_SCHOOL_ED_FIELD,\n",
    "    field_names.MEDIAN_INCOME_FIELD,\n",
    "    field_names.URBAN_HEURISTIC_FIELD,\n",
    "    field_names.LIFE_EXPECTANCY_FIELD,\n",
    "    field_names.HEALTH_INSURANCE_FIELD,\n",
    "    field_names.PHYS_HEALTH_NOT_GOOD_FIELD,\n",
    "    field_names.DIABETES_FIELD,\n",
    "    field_names.LOW_READING_FIELD + field_names.PERCENTILE_FIELD_SUFFIX,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b510cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_state_distributions(\n",
    "    df: pd.DataFrame, priority_communities_fields: typing.List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"For each boolean field of priority communities, calculate distribution across states and territories.\"\"\"\n",
    "\n",
    "    # Ensure each field is boolean.\n",
    "    for priority_communities_field in priority_communities_fields:\n",
    "        if df[priority_communities_field].dtype != bool:\n",
    "            print(f\"Converting {priority_communities_field} to boolean.\")\n",
    "\n",
    "        # Calculate the population included as priority communities per tract. Will either be 0 or the population.\n",
    "        df[f\"{priority_communities_field}{POPULATION_SUFFIX}\"] = (\n",
    "            df[priority_communities_field]\n",
    "            * df[field_names.COMBINED_CENSUS_TOTAL_POPULATION_2010]\n",
    "        )\n",
    "\n",
    "    def calculate_state_comparison(\n",
    "        frame: pd.DataFrame, geography_field: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method will be applied to a `group_by` object. Inherits some parameters from outer scope.\n",
    "\n",
    "        \"\"\"\n",
    "        summary_dict = {}\n",
    "        summary_dict[COUNTRY_FIELD_NAME] = frame[COUNTRY_FIELD_NAME].unique()[0]\n",
    "\n",
    "        summary_dict[\"Analysis grouped by\"] = geography_field\n",
    "\n",
    "        if geography_field == COUNTRY_FIELD_NAME:\n",
    "            summary_dict[GEOID_STATE_FIELD_NAME] = \"00\"\n",
    "            summary_dict[\"Geography name\"] = \"(Entire USA)\"\n",
    "\n",
    "        if geography_field == GEOID_STATE_FIELD_NAME:\n",
    "            state_id = frame[GEOID_STATE_FIELD_NAME].unique()[0]\n",
    "            summary_dict[GEOID_STATE_FIELD_NAME] = state_id\n",
    "            summary_dict[\"Geography name\"] = us.states.lookup(state_id).name\n",
    "\n",
    "            # Also add region information\n",
    "            region_id = frame[\"region\"].unique()[0]\n",
    "            summary_dict[\"region\"] = region_id\n",
    "\n",
    "        if geography_field == \"region\":\n",
    "            region_id = frame[\"region\"].unique()[0]\n",
    "            summary_dict[\"region\"] = region_id\n",
    "            summary_dict[\"Geography name\"] = region_id\n",
    "\n",
    "        if geography_field == \"division\":\n",
    "            division_id = frame[\"division\"].unique()[0]\n",
    "            summary_dict[\"division\"] = division_id\n",
    "            summary_dict[\"Geography name\"] = division_id\n",
    "\n",
    "        total_tracts_in_geography = len(frame)\n",
    "        total_population_in_geography = frame[\n",
    "            field_names.COMBINED_CENSUS_TOTAL_POPULATION_2010\n",
    "        ].sum()\n",
    "\n",
    "        if geography_field == field_names.URBAN_HEURISTIC_FIELD:\n",
    "            urban_flag = frame[field_names.URBAN_HEURISTIC_FIELD].unique()[0]\n",
    "            summary_dict[\"Urban vs Rural\"] = \"Urban\" if urban_flag else \"Rural\"\n",
    "            summary_dict[\"Geography name\"] = summary_dict[\"Urban vs Rural\"]\n",
    "\n",
    "        for priority_communities_field in priority_communities_fields:\n",
    "            summary_dict[\n",
    "                f\"{priority_communities_field}{POPULATION_SUFFIX}\"\n",
    "            ] = frame[f\"{priority_communities_field}{POPULATION_SUFFIX}\"].sum()\n",
    "\n",
    "            summary_dict[\n",
    "                f\"{priority_communities_field} (total tracts)\"\n",
    "            ] = frame[f\"{priority_communities_field}\"].sum()\n",
    "\n",
    "            # Calculate some combinations of other variables.\n",
    "            summary_dict[f\"{priority_communities_field} (percent tracts)\"] = (\n",
    "                summary_dict[f\"{priority_communities_field} (total tracts)\"]\n",
    "                / total_tracts_in_geography\n",
    "            )\n",
    "\n",
    "            summary_dict[\n",
    "                f\"{priority_communities_field} (percent population)\"\n",
    "            ] = (\n",
    "                summary_dict[f\"{priority_communities_field}{POPULATION_SUFFIX}\"]\n",
    "                / total_population_in_geography\n",
    "            )\n",
    "\n",
    "            unwanted_keys = [\n",
    "                f\"{priority_communities_field}{POPULATION_SUFFIX}\",\n",
    "                f\"{priority_communities_field} (total tracts)\",\n",
    "            ]\n",
    "\n",
    "            # Remove unneeded columns:\n",
    "            for unwanted_key in unwanted_keys:\n",
    "                del summary_dict[unwanted_key]\n",
    "\n",
    "        df = pd.DataFrame(summary_dict, index=[0])\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Add a field for country so we can do aggregations across the entire country.\n",
    "    df[COUNTRY_FIELD_NAME] = \"USA\"\n",
    "\n",
    "    # First, run the comparison by the whole country\n",
    "    usa_grouped_df = df.groupby(COUNTRY_FIELD_NAME)\n",
    "\n",
    "    # Run the comparison function on the groups.\n",
    "    usa_distribution_df = usa_grouped_df.progress_apply(\n",
    "        lambda frame: calculate_state_comparison(\n",
    "            frame, geography_field=COUNTRY_FIELD_NAME\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Next, run the comparison by state\n",
    "    state_grouped_df = df.groupby(GEOID_STATE_FIELD_NAME)\n",
    "\n",
    "    # Run the comparison function on the groups.\n",
    "    state_distribution_df = state_grouped_df.progress_apply(\n",
    "        lambda frame: calculate_state_comparison(\n",
    "            frame, geography_field=GEOID_STATE_FIELD_NAME\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Next, run the comparison by region\n",
    "    region_grouped_df = df.groupby(\"region\")\n",
    "\n",
    "    # Run the comparison function on the groups.\n",
    "    region_distribution_df = region_grouped_df.progress_apply(\n",
    "        lambda frame: calculate_state_comparison(\n",
    "            frame, geography_field=\"region\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Next, run the comparison by division\n",
    "    division_grouped_df = df.groupby(\"division\")\n",
    "\n",
    "    # Run the comparison function on the groups.\n",
    "    division_distribution_df = division_grouped_df.progress_apply(\n",
    "        lambda frame: calculate_state_comparison(\n",
    "            frame, geography_field=\"division\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Next, run the comparison by urban/rural\n",
    "    urban_grouped_df = df.groupby(field_names.URBAN_HEURISTIC_FIELD)\n",
    "\n",
    "    # Run the comparison function on the groups.\n",
    "    urban_grouped_df = urban_grouped_df.progress_apply(\n",
    "        lambda frame: calculate_state_comparison(\n",
    "            frame, geography_field=field_names.URBAN_HEURISTIC_FIELD\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Combine the five\n",
    "    combined_df = pd.concat(\n",
    "        [\n",
    "            usa_distribution_df,\n",
    "            state_distribution_df,\n",
    "            region_distribution_df,\n",
    "            division_distribution_df,\n",
    "            urban_grouped_df,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def write_state_distribution_excel(\n",
    "    state_distribution_df: pd.DataFrame, file_path: pathlib.PosixPath\n",
    ") -> None:\n",
    "    \"\"\"Write the dataframe to excel with special formatting.\"\"\"\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(file_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    # Convert the dataframe to an XlsxWriter Excel object. We also turn off the\n",
    "    # index column at the left of the output dataframe.\n",
    "    state_distribution_df.to_excel(writer, sheet_name=\"Sheet1\", index=False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "    worksheet.autofilter(\n",
    "        0, 0, state_distribution_df.shape[0], state_distribution_df.shape[1]\n",
    "    )\n",
    "\n",
    "    # Set a width parameter for all columns\n",
    "    # Note: this is parameterized because every call to `set_column` requires setting the width.\n",
    "    column_width = 15\n",
    "\n",
    "    for column in state_distribution_df.columns:\n",
    "        # Turn the column index into excel ranges (e.g., column #95 is \"CR\" and the range may be \"CR2:CR53\").\n",
    "        column_index = state_distribution_df.columns.get_loc(column)\n",
    "        column_character = get_excel_column_name(column_index)\n",
    "\n",
    "        # Set all columns to larger width\n",
    "        worksheet.set_column(\n",
    "            f\"{column_character}:{column_character}\", column_width\n",
    "        )\n",
    "\n",
    "        # Special formatting for all percent columns\n",
    "        # Note: we can't just search for `percent`, because that's included in the word `percentile`.\n",
    "        if \"percent \" in column or \"(percent)\" in column:\n",
    "            # Make these columns percentages.\n",
    "            percentage_format = workbook.add_format({\"num_format\": \"0%\"})\n",
    "            worksheet.set_column(\n",
    "                f\"{column_character}:{column_character}\",\n",
    "                column_width,\n",
    "                percentage_format,\n",
    "            )\n",
    "\n",
    "        # Special formatting for columns that capture the percent of population considered priority.\n",
    "        if \"(percent population)\" in column:\n",
    "            column_ranges = f\"{column_character}2:{column_character}{len(state_distribution_df)+1}\"\n",
    "\n",
    "            # Add green to red conditional formatting.\n",
    "            worksheet.conditional_format(\n",
    "                column_ranges,\n",
    "                # Min: green, max: red.\n",
    "                {\n",
    "                    \"type\": \"2_color_scale\",\n",
    "                    \"min_color\": \"#00FF7F\",\n",
    "                    \"max_color\": \"#C82538\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "    header_format = workbook.add_format(\n",
    "        {\"bold\": True, \"text_wrap\": True, \"valign\": \"bottom\"}\n",
    "    )\n",
    "\n",
    "    # Overwrite both the value and the format of each header cell\n",
    "    # This is because xlsxwriter / pandas has a known bug where it can't wrap text for a dataframe.\n",
    "    # See https://stackoverflow.com/questions/42562977/xlsxwriter-text-wrap-not-working.\n",
    "    for col_num, value in enumerate(state_distribution_df.columns.values):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "fields_to_analyze = [\n",
    "    index.priority_communities_field for index in census_tract_indices\n",
    "]\n",
    "\n",
    "# Convert all indices to boolean\n",
    "for field_to_analyze in fields_to_analyze:\n",
    "    if \"Areas of Concern\" in field_to_analyze:\n",
    "        print(f\"Converting {field_to_analyze} to boolean.\")\n",
    "\n",
    "        merged_df[field_to_analyze] = merged_df[field_to_analyze].fillna(\n",
    "            value=0\n",
    "        )\n",
    "        merged_df[field_to_analyze] = merged_df[field_to_analyze].astype(bool)\n",
    "\n",
    "\n",
    "state_fips_codes = get_state_information(DATA_DIR)\n",
    "\n",
    "merged_with_state_information_df = merged_df.merge(\n",
    "    right=state_fips_codes, left_on=GEOID_STATE_FIELD_NAME, right_on=\"fips\"\n",
    ")\n",
    "\n",
    "state_distribution_df = get_state_distributions(\n",
    "    df=merged_with_state_information_df,\n",
    "    priority_communities_fields=fields_to_analyze,\n",
    ")\n",
    "\n",
    "file_prefix = \"Priority Tracts – Different geographic groupings\"\n",
    "\n",
    "state_distribution_df.to_csv(\n",
    "    path_or_buf=COMPARISON_OUTPUTS_DIR / f\"{file_prefix}.csv\",\n",
    "    na_rep=\"\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "write_state_distribution_excel(\n",
    "    state_distribution_df=state_distribution_df,\n",
    "    file_path=COMPARISON_OUTPUTS_DIR / f\"{file_prefix}.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbcabf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = COMPARISON_OUTPUTS_DIR / \"tracts_basic_stats\"\n",
    "directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TODO: this Excel-writing function is extremely similar to other Excel-writing functions in this notebook.\n",
    "# Refactor to use the same Excel-writing function.\n",
    "def write_basic_stats_excel(\n",
    "    basic_stats_df: pd.DataFrame, file_path: pathlib.PosixPath\n",
    ") -> None:\n",
    "    \"\"\"Write the dataframe to excel with special formatting.\"\"\"\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(file_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    # Convert the dataframe to an XlsxWriter Excel object. We also turn off the\n",
    "    # index column at the left of the output dataframe.\n",
    "    basic_stats_df.to_excel(writer, sheet_name=\"Sheet1\", index=False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "    worksheet.autofilter(0, 0, basic_stats_df.shape[0], basic_stats_df.shape[1])\n",
    "\n",
    "    # Set a width parameter for all columns\n",
    "    # Note: this is parameterized because every call to `set_column` requires setting the width.\n",
    "    column_width = 15\n",
    "\n",
    "    for column in basic_stats_df.columns:\n",
    "        # Turn the column index into excel ranges (e.g., column #95 is \"CR\" and the range may be \"CR2:CR53\").\n",
    "        column_index = basic_stats_df.columns.get_loc(column)\n",
    "        column_character = get_excel_column_name(column_index)\n",
    "\n",
    "        # Set all columns to larger width\n",
    "        worksheet.set_column(\n",
    "            f\"{column_character}:{column_character}\", column_width\n",
    "        )\n",
    "\n",
    "        # Add green to red conditional formatting.\n",
    "        column_ranges = (\n",
    "            f\"{column_character}2:{column_character}{len(basic_stats_df)+1}\"\n",
    "        )\n",
    "        worksheet.conditional_format(\n",
    "            column_ranges,\n",
    "            # Min: green, max: red.\n",
    "            {\n",
    "                \"type\": \"2_color_scale\",\n",
    "                \"min_color\": \"#00FF7F\",\n",
    "                \"max_color\": \"#C82538\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Special formatting for all percent columns\n",
    "        # Note: we can't just search for `percent`, because that's included in the word `percentile`.\n",
    "        if (\n",
    "            \"percent \" in column\n",
    "            or \"(percent)\" in column\n",
    "            or \"Percent \" in column\n",
    "        ):\n",
    "            # Make these columns percentages.\n",
    "            percentage_format = workbook.add_format({\"num_format\": \"0%\"})\n",
    "            worksheet.set_column(\n",
    "                f\"{column_character}:{column_character}\",\n",
    "                column_width,\n",
    "                percentage_format,\n",
    "            )\n",
    "\n",
    "    header_format = workbook.add_format(\n",
    "        {\"bold\": True, \"text_wrap\": True, \"valign\": \"bottom\"}\n",
    "    )\n",
    "\n",
    "    # Overwrite both the value and the format of each header cell\n",
    "    # This is because xlsxwriter / pandas has a known bug where it can't wrap text for a dataframe.\n",
    "    # See https://stackoverflow.com/questions/42562977/xlsxwriter-text-wrap-not-working.\n",
    "    for col_num, value in enumerate(basic_stats_df.columns.values):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "for index in census_tract_indices:\n",
    "    print(f\"Basic stats for {index.method_name}\")\n",
    "    temp_df = merged_df\n",
    "    temp_df[index.priority_communities_field] = (\n",
    "        temp_df[index.priority_communities_field] == True\n",
    "    )\n",
    "\n",
    "    grouped_df = (\n",
    "        temp_df.groupby(index.priority_communities_field).mean().reset_index()\n",
    "    )\n",
    "    result_df = grouped_df[\n",
    "        [index.priority_communities_field] + comparison_fields\n",
    "    ]\n",
    "    result_df.to_csv(\n",
    "        directory / f\"{index.method_name} Basic Stats.csv\", index=False\n",
    "    )\n",
    "    write_basic_stats_excel(\n",
    "        basic_stats_df=result_df,\n",
    "        file_path=directory / f\"{index.method_name} Basic Stats.xlsx\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eec560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare census tract scores to each other, running secondary analysis on\n",
    "# characteristics of census tracts prioritized by one but not the other.\n",
    "def get_census_tracts_score_comparison_df(\n",
    "    df: pd.DataFrame,\n",
    "    method_a_priority_census_tracts_field: str,\n",
    "    method_b_priority_census_tracts_field: str,\n",
    "    comparison_fields: typing.List[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare tract scores to each other.\n",
    "\n",
    "    This comparison method analyzes characteristics of those census tracts, based on whether or not they are prioritized\n",
    "    or not by Method A and/or Method B.\n",
    "\n",
    "    E.g., it might show that tracts prioritized by A but not B have a higher average income,\n",
    "    or that tracts prioritized by B but not A have a lower percent of unemployed people.\n",
    "    \"\"\"\n",
    "    fields_to_group_by = [\n",
    "        method_a_priority_census_tracts_field,\n",
    "        method_b_priority_census_tracts_field,\n",
    "    ]\n",
    "\n",
    "    df_subset = df[fields_to_group_by + comparison_fields]\n",
    "\n",
    "    grouped_df = df_subset.groupby(\n",
    "        fields_to_group_by,\n",
    "        dropna=False,\n",
    "    )\n",
    "\n",
    "    # Take the mean of all fields.\n",
    "    comparison_df = grouped_df.mean()\n",
    "\n",
    "    # Also add in the count of census tracts.\n",
    "    count_field_name = \"Count of census tracts\"\n",
    "    comparison_df[count_field_name] = grouped_df.size().to_frame(\n",
    "        count_field_name\n",
    "    )\n",
    "\n",
    "    comparison_df = comparison_df.reset_index()\n",
    "\n",
    "    criteria_description_field_name = \"Description of criteria\"\n",
    "    comparison_df[criteria_description_field_name] = comparison_df.apply(\n",
    "        func=lambda row: f\"Tracts that are {'not ' if row[method_a_priority_census_tracts_field] is False else ''}\"\n",
    "        + f\"prioritized by {method_a_priority_census_tracts_field} \"\n",
    "        + f\"and are {'not ' if row[method_b_priority_census_tracts_field] is False else ''}\"\n",
    "        + f\"prioritized by {method_b_priority_census_tracts_field}\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Put criteria description column first.\n",
    "    columns_to_put_first = (\n",
    "        [criteria_description_field_name]\n",
    "        + fields_to_group_by\n",
    "        + [count_field_name]\n",
    "    )\n",
    "    new_column_order = columns_to_put_first + [\n",
    "        col for col in comparison_df.columns if col not in columns_to_put_first\n",
    "    ]\n",
    "\n",
    "    comparison_df = comparison_df[new_column_order]\n",
    "\n",
    "    # Rename fields to reflect the mean aggregation\n",
    "    comparison_df.rename(\n",
    "        mapper={\n",
    "            comparison_field: f\"{comparison_field} (mean of tracts)\"\n",
    "            for comparison_field in comparison_fields\n",
    "        },\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def write_census_tracts_score_comparison_excel(\n",
    "    census_tracts_score_comparison_df: pd.DataFrame,\n",
    "    file_path: pathlib.PosixPath,\n",
    ") -> None:\n",
    "    \"\"\"Write the dataframe to excel with special formatting.\"\"\"\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(file_path, engine=\"xlsxwriter\")\n",
    "\n",
    "    # Convert the dataframe to an XlsxWriter Excel object. We also turn off the\n",
    "    # index column at the left of the output dataframe.\n",
    "    census_tracts_score_comparison_df.to_excel(\n",
    "        writer, sheet_name=\"Sheet1\", index=False\n",
    "    )\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "    worksheet.autofilter(\n",
    "        0,\n",
    "        0,\n",
    "        census_tracts_score_comparison_df.shape[0],\n",
    "        census_tracts_score_comparison_df.shape[1],\n",
    "    )\n",
    "\n",
    "    # Set a width parameter for all columns\n",
    "    # Note: this is parameterized because every call to `set_column` requires setting the width.\n",
    "    column_width = 15\n",
    "\n",
    "    for column in census_tracts_score_comparison_df.columns:\n",
    "        # Turn the column index into excel ranges (e.g., column #95 is \"CR\" and the range may be \"CR2:CR53\").\n",
    "        column_index = census_tracts_score_comparison_df.columns.get_loc(column)\n",
    "        column_character = get_excel_column_name(column_index)\n",
    "\n",
    "        # Set all columns to larger width\n",
    "        worksheet.set_column(\n",
    "            f\"{column_character}:{column_character}\", column_width\n",
    "        )\n",
    "\n",
    "        # Add green to red conditional formatting.\n",
    "        column_ranges = f\"{column_character}2:{column_character}{len(census_tracts_score_comparison_df)+1}\"\n",
    "        worksheet.conditional_format(\n",
    "            column_ranges,\n",
    "            # Min: green, max: red.\n",
    "            {\n",
    "                \"type\": \"2_color_scale\",\n",
    "                \"min_color\": \"#00FF7F\",\n",
    "                \"max_color\": \"#C82538\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Special formatting for all percent columns\n",
    "        # Note: we can't just search for `percent`, because that's included in the word `percentile`.\n",
    "        if (\n",
    "            \"percent \" in column\n",
    "            or \"(percent)\" in column\n",
    "            or \"Percent \" in column\n",
    "        ):\n",
    "            # Make these columns percentages.\n",
    "            percentage_format = workbook.add_format({\"num_format\": \"0%\"})\n",
    "            worksheet.set_column(\n",
    "                f\"{column_character}:{column_character}\",\n",
    "                column_width,\n",
    "                percentage_format,\n",
    "            )\n",
    "\n",
    "    header_format = workbook.add_format(\n",
    "        {\"bold\": True, \"text_wrap\": True, \"valign\": \"bottom\"}\n",
    "    )\n",
    "\n",
    "    # Overwrite both the value and the format of each header cell\n",
    "    # This is because xlsxwriter / pandas has a known bug where it can't wrap text for a dataframe.\n",
    "    # See https://stackoverflow.com/questions/42562977/xlsxwriter-text-wrap-not-working.\n",
    "    for col_num, value in enumerate(\n",
    "        census_tracts_score_comparison_df.columns.values\n",
    "    ):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "def compare_census_tracts_scores(\n",
    "    df: pd.DataFrame,\n",
    "    index_a: Index,\n",
    "    index_b: Index,\n",
    "    output_dir: pathlib.PosixPath,\n",
    "    comparison_fields: typing.List[str],\n",
    "):\n",
    "    # Secondary comparison DF\n",
    "    census_tracts_score_comparison_df = get_census_tracts_score_comparison_df(\n",
    "        df=df,\n",
    "        method_a_priority_census_tracts_field=index_a.priority_communities_field,\n",
    "        method_b_priority_census_tracts_field=index_b.priority_communities_field,\n",
    "        comparison_fields=comparison_fields,\n",
    "    )\n",
    "\n",
    "    # Write secondary comparison to CSV.\n",
    "    file_name_part = f\"Census tracts comparison output - {index_a.method_name} and {index_b.method_name}\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = output_dir / (file_name_part + \".csv\")\n",
    "    file_path_xlsx = output_dir / (file_name_part + \".xlsx\")\n",
    "\n",
    "    census_tracts_score_comparison_df.to_csv(\n",
    "        path_or_buf=file_path,\n",
    "        na_rep=\"\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    write_census_tracts_score_comparison_excel(\n",
    "        census_tracts_score_comparison_df=census_tracts_score_comparison_df,\n",
    "        file_path=file_path_xlsx,\n",
    "    )\n",
    "\n",
    "\n",
    "for (index_a, index_b) in itertools.combinations(census_tract_indices, 2):\n",
    "    print(f\"Comparing {index_a} and {index_b}.\")\n",
    "    compare_census_tracts_scores(\n",
    "        df=merged_df,\n",
    "        index_a=index_a,\n",
    "        index_b=index_b,\n",
    "        comparison_fields=comparison_fields,\n",
    "        output_dir=COMPARISON_OUTPUTS_DIR / \"census_tracts_score_comparisons\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48005fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_markdown_and_docx_content(\n",
    "    markdown_content: str,\n",
    "    file_dir: pathlib.PosixPath,\n",
    "    file_name_without_extension: str,\n",
    ") -> pathlib.PosixPath:\n",
    "    \"\"\"Write Markdown content to both .md and .docx files.\"\"\"\n",
    "    # Set the file paths for both files.\n",
    "    markdown_file_path = file_dir / f\"{file_name_without_extension}.md\"\n",
    "    docx_file_path = file_dir / f\"{file_name_without_extension}.docx\"\n",
    "\n",
    "    # Write the markdown content to file.\n",
    "    with open(markdown_file_path, \"w\") as text_file:\n",
    "        text_file.write(markdown_content)\n",
    "\n",
    "    # Convert markdown file to Word doc.\n",
    "    pypandoc.convert_file(\n",
    "        source_file=str(markdown_file_path),\n",
    "        to=\"docx\",\n",
    "        outputfile=str(docx_file_path),\n",
    "        extra_args=[],\n",
    "    )\n",
    "\n",
    "    return docx_file_path\n",
    "\n",
    "\n",
    "def get_markdown_comparing_census_tract_indices(\n",
    "    census_tract_indices=typing.List[Index],\n",
    "    df=pd.DataFrame,\n",
    "    state_field=GEOID_STATE_FIELD_NAME,\n",
    ") -> str:\n",
    "    \"\"\"Generate a Markdown string of analysis of multiple census tract indices.\"\"\"\n",
    "    count_field_name = \"Count of census tracts\"\n",
    "\n",
    "    # Create markdown content for comparisons.\n",
    "    markdown_content = f\"\"\"\n",
    "# Comparing multiple indices at the census tract level\n",
    "\n",
    "(This report was calculated on {datetime.today().strftime('%Y-%m-%d')}.)\n",
    "\n",
    "This report compares the following indices: {\", \".join([index.method_name for index in census_tract_indices])}.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    for (index1, index2) in itertools.combinations(census_tract_indices, 2):\n",
    "        # First, find out geographic overlap in indices by finding all state and territory\n",
    "        # names where both indices are not null.\n",
    "        df_subset_for_states = df[\n",
    "            [\n",
    "                state_field,\n",
    "                index1.priority_communities_field,\n",
    "                index2.priority_communities_field,\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        df_subset_for_states = df_subset_for_states.dropna()\n",
    "\n",
    "        # List of all states/territories in their FIPS codes:\n",
    "        # TODO: move \"This report analyzes the following US states and territories\" inside the comparison?\n",
    "        state_ids = sorted(df_subset_for_states[state_field].unique())\n",
    "        this_comparison_state_names = \", \".join(\n",
    "            [us.states.lookup(state_id).name for state_id in state_ids]\n",
    "        )\n",
    "\n",
    "        # Group all data by their different values on Priority Communities Field for Index1 vs Priority Communities Field for Index2.\n",
    "        count_df = (\n",
    "            df.groupby(\n",
    "                [\n",
    "                    index1.priority_communities_field,\n",
    "                    index2.priority_communities_field,\n",
    "                ]\n",
    "            )[ExtractTransformLoad.GEOID_TRACT_FIELD_NAME]\n",
    "            .count()\n",
    "            .reset_index(name=count_field_name)\n",
    "        )\n",
    "\n",
    "        total_census_tracts = count_df[count_field_name].sum()\n",
    "\n",
    "        # Returns a series\n",
    "        true_true_census_tracts_series = count_df.loc[\n",
    "            count_df[index1.priority_communities_field]\n",
    "            & count_df[index2.priority_communities_field],\n",
    "            count_field_name,\n",
    "        ]\n",
    "        true_false_census_tracts_series = count_df.loc[\n",
    "            count_df[index1.priority_communities_field]\n",
    "            & ~count_df[index2.priority_communities_field],\n",
    "            count_field_name,\n",
    "        ]\n",
    "        false_true_census_tracts_series = count_df.loc[\n",
    "            ~count_df[index1.priority_communities_field]\n",
    "            & count_df[index2.priority_communities_field],\n",
    "            count_field_name,\n",
    "        ]\n",
    "        false_false_census_tracts_series = count_df.loc[\n",
    "            ~count_df[index1.priority_communities_field]\n",
    "            & ~count_df[index2.priority_communities_field],\n",
    "            count_field_name,\n",
    "        ]\n",
    "\n",
    "        # Convert from series to a scalar value, including accounting for if no data exists for that pairing.\n",
    "        true_true_census_tracts = (\n",
    "            true_true_census_tracts_series.iloc[0]\n",
    "            if len(true_true_census_tracts_series) > 0\n",
    "            else 0\n",
    "        )\n",
    "        true_false_census_tracts = (\n",
    "            true_false_census_tracts_series.iloc[0]\n",
    "            if len(true_false_census_tracts_series) > 0\n",
    "            else 0\n",
    "        )\n",
    "        false_true_census_tracts = (\n",
    "            false_true_census_tracts_series.iloc[0]\n",
    "            if len(false_true_census_tracts_series) > 0\n",
    "            else 0\n",
    "        )\n",
    "        false_false_census_tracts = (\n",
    "            false_false_census_tracts_series.iloc[0]\n",
    "            if len(false_false_census_tracts_series) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        markdown_content += (\n",
    "            \"*** \\n\\n\"\n",
    "            \"There are \"\n",
    "            f\"{true_true_census_tracts} ({true_true_census_tracts / total_census_tracts:.0%}) \"\n",
    "            f\"census tracts that are both {index1.method_name} priority communities and {index2.method_name} priority communities.\\n\\n\"\n",
    "            \"There are \"\n",
    "            f\"{true_false_census_tracts} ({true_false_census_tracts / total_census_tracts:.0%}) \"\n",
    "            f\"census tracts that are {index1.method_name} priority communities but not {index2.method_name} priority communities.\\n\\n\"\n",
    "            \"There are \"\n",
    "            f\"{false_true_census_tracts} ({false_true_census_tracts / total_census_tracts:.0%}) \"\n",
    "            f\"census tracts that are not {index1.method_name} priority communities but are {index2.method_name} priority communities.\\n\\n\"\n",
    "            \"There are \"\n",
    "            f\"{false_false_census_tracts} ({false_false_census_tracts / total_census_tracts:.0%}) \"\n",
    "            f\"census tracts that are neither {index1.method_name} priority communities nor {index2.method_name} priority communities.\\n\\n\"\n",
    "            f\"This comparison analyzed the following US states and territories: {this_comparison_state_names}.\\n\\n\"\n",
    "            \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "def get_comparison_census_tract_indices(\n",
    "    census_tract_indices=typing.List[Index],\n",
    "    df=pd.DataFrame,\n",
    "    state_field=GEOID_STATE_FIELD_NAME,\n",
    ") -> pathlib.PosixPath:\n",
    "    markdown_content = get_markdown_comparing_census_tract_indices(\n",
    "        census_tract_indices=census_tract_indices,\n",
    "        df=df,\n",
    "    )\n",
    "\n",
    "    comparison_docx_file_path = write_markdown_and_docx_content(\n",
    "        markdown_content=markdown_content,\n",
    "        file_dir=COMPARISON_OUTPUTS_DIR,\n",
    "        file_name_without_extension=f\"Comparison report - All census tract indices\",\n",
    "    )\n",
    "\n",
    "    return comparison_docx_file_path\n",
    "\n",
    "\n",
    "# Compare multiple scores at the census tract level\n",
    "get_comparison_census_tract_indices(\n",
    "    census_tract_indices=census_tract_indices,\n",
    "    df=merged_with_state_information_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d095ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: this is helpful because this file is long-running, so it alerts the user when the\n",
    "# data analysis is done. Can be removed when converted into scripts. -LMB.\n",
    "import os\n",
    "\n",
    "os.system(\"say 'data analysis is written.'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
